import numpy as np
import trimesh
from scipy.spatial import KDTree
from trimesh import smoothing
import pymeshfix
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms

def barycentric_coords(P, A, B, C):
    """
    Calculate the barycentric coordinates of point P with respect to triangle ABC (vectorized implementation)
    P, A, B, C are all arrays of shape (n,3), return u,v,w, satisfying u+v+w=1
    """
    v0 = B - A
    v1 = C - A
    v2 = P - A
    d00 = np.sum(v0 * v0, axis=1)
    d01 = np.sum(v0 * v1, axis=1)
    d11 = np.sum(v1 * v1, axis=1)
    d20 = np.sum(v2 * v0, axis=1)
    d21 = np.sum(v2 * v1, axis=1)
    denom = d00 * d11 - d01 * d01
    # Avoid division by zero
    denom[denom == 0] = 1e-8
    v = (d11 * d20 - d01 * d21) / denom
    w = (d00 * d21 - d01 * d20) / denom
    u = 1.0 - v - w
    return u, v, w

def extract_texture_patch(ply_filename, texture_filename, len_pixel=128, smooth_method="laplacian", smooth_iterations=10):

    # Load mesh and texture image and preprocess
    mesh = trimesh.load(ply_filename, process=False)
    face_normals = mesh.face_normals
    texture_img = Image.open(texture_filename).convert('RGB')
    texture_array = np.array(texture_img)
    
    # if smooth, repair and smooth the mesh, then update the face normals
    if smooth_method is not None:
        meshfix = pymeshfix.MeshFix(mesh.vertices, mesh.faces)
        meshfix.repair()
        mesh_repaired = trimesh.Trimesh(vertices=meshfix.v, faces=meshfix.f)

        if mesh_repaired.is_watertight:
            print("PyMeshFix repair successful, the mesh is now closed.")
        else:
            print("The mesh still has problems after repair.")
    
        print(f"Applying {smooth_method} smoothing with {smooth_iterations} iterations...")
        if smooth_method == "taubin": 
            # Taubin smoothing, tend to keep sharper edges
            mesh_smoothed = smoothing.filter_taubin(mesh_repaired, iterations=smooth_iterations) 
        elif smooth_method == "laplacian": 
            # Laplacian smoothing, tend to make the mesh smoother
            mesh_smoothed = smoothing.filter_laplacian(mesh_repaired, iterations=smooth_iterations)
        else:
            print(f"Unknown smoothing method: {smooth_method}. Skipping smoothing.")
            mesh_smoothed = mesh_repaired 
            
        # Establish the correspondence between the original mesh and the repaired mesh vertices
        original_vertices = mesh.vertices
        repaired_vertices = mesh_repaired.vertices

        # Use KDTree for fast nearest neighbor search
        tree = KDTree(repaired_vertices)
        vertex_mapping = []
        for v in original_vertices:
            distance, idx = tree.query(v)
            vertex_mapping.append(idx)
        # vertex_mapping represents the index of the corresponding vertex in mesh_repaired/mesh_smoothed for the i-th vertex in mesh

        # Recalculate the face normals based on the smoothed vertices
        smoothed_vertices = mesh_smoothed.vertices
        original_face_indices = mesh.faces.copy()
        new_face_normals = []
        for face in original_face_indices:
            idx0 = vertex_mapping[face[0]]
            idx1 = vertex_mapping[face[1]]
            idx2 = vertex_mapping[face[2]]
            v0 = smoothed_vertices[idx0]
            v1 = smoothed_vertices[idx1]
            v2 = smoothed_vertices[idx2]
            
            edge1 = v1 - v0
            edge2 = v2 - v0
            
            normal = np.cross(edge1, edge2)
            norm_val = np.linalg.norm(normal)
            if norm_val > 1e-8:
                normal = normal / norm_val
            new_face_normals.append(normal)
        new_face_normals = np.array(new_face_normals)
        print("Updated face normals computed based on smoothed vertices.")
    
    # Calculate s and select a random surface sampling point P and its normal
    bbox = mesh.bounds  # shape (2,3)
    object_size = np.linalg.norm(bbox[1] - bbox[0])
    s = object_size / 10.0
    sample_points, face_indices = trimesh.sample.sample_surface(mesh, count=1000)
    idx = np.random.choice(len(sample_points))
    P = sample_points[idx]
    chosen_face_idx = face_indices[idx]
    normal = face_normals[chosen_face_idx]
    
    list_patch_color_img = []
    list_patch_mask_img = []
    list_patch_mask_img_inpainted = []
    
    patch_color_img, patch_mask_img, patch_mask_img_inpainted = extract_texture_patch_at_point(normal, s, object_size, P, mesh, texture_array, len_pixel, plot=True)
    
    result = extract_vgg_features(patch_color_img, N=10, stats_type='sum')
    for key, value in result.items():
        print(f"{key}: {value}")
        
    list_patch_color_img.append(patch_color_img)
    list_patch_mask_img.append(patch_mask_img)
    list_patch_mask_img_inpainted.append(patch_mask_img_inpainted)
    
    return list_patch_color_img, list_patch_mask_img, list_patch_mask_img_inpainted

def extract_texture_patch_at_point(normal, s, object_size, P, mesh, texture_array, len_pixel=128, plot=False):
    
    # Construct a local plane base T1, T2 perpendicular to the normal vector
    if abs(normal[2]) < 0.9:
        ref = np.array([0, 0, 1])
    else:
        ref = np.array([0, 1, 0])
    T1 = np.cross(normal, ref)
    T1 = T1 / np.linalg.norm(T1)
    T2 = np.cross(normal, T1)
    T2 = T2 / np.linalg.norm(T2)
    
    # Create a square sampling area covering 2s x 2s (image size is 2*len_pixel x 2*len_pixel)
    grid_size = 2 * len_pixel
    xs = np.linspace(-s, s, grid_size)
    ys = np.linspace(-s, s, grid_size)
    grid_X, grid_Y = np.meshgrid(xs, ys)
    grid_X_flat = grid_X.ravel()
    grid_Y_flat = grid_Y.ravel()
    num_points = grid_X_flat.shape[0]
    
    # Only select points within a distance of s from the center
    distances_2d = np.sqrt(grid_X_flat**2 + grid_Y_flat**2)
    valid_circle = distances_2d <= s
    
    # Move the sampling plane origin from P along the normal by object_size, and intersect along the -normal ray
    # Calculate the 3D coordinates of each sampling point: Q = projection_origin + x*T1 + y*T2
    projection_origin = P + object_size * normal
    origins = projection_origin + np.outer(grid_X_flat, T1) + np.outer(grid_Y_flat, T2)
    
    # All ray directions are -normal
    ray_direction = -normal
    ray_directions = np.tile(ray_direction, (num_points, 1))
    
    # Only emit rays for sampling points in the valid area
    valid_indices = np.where(valid_circle)[0]
    valid_origins = origins[valid_indices]
    valid_ray_directions = ray_directions[valid_indices]
    
    # Use intersects_location to get all intersection information
    locations, index_ray, index_tri = mesh.ray.intersects_location(
        ray_origins=valid_origins, 
        ray_directions=valid_ray_directions,
        multiple_hits=False
    )
    
    hit_points = locations
    hit_face_indices = index_tri

    # Calculate the UV coordinates of the intersection point using barycentric interpolation and sample the texture color
    faces = mesh.faces[hit_face_indices]
    A = mesh.vertices[faces[:, 0]]
    B = mesh.vertices[faces[:, 1]]
    C = mesh.vertices[faces[:, 2]]
    
    # calculates the barycentric coordinates
    u, v, w = barycentric_coords(hit_points, A, B, C)
    if hasattr(mesh.visual, 'uv') and mesh.visual.uv is not None:
        vertex_uv = mesh.visual.uv
    else:
        raise ValueError("Failed to read texture UV information!")
    
    uvA = vertex_uv[faces[:, 0]]
    uvB = vertex_uv[faces[:, 1]]
    uvC = vertex_uv[faces[:, 2]]
    hit_uv = u[:, None] * uvA + v[:, None] * uvB + w[:, None] * uvC
    
    # Sample the texture color using vectorized operations
    tex_height, tex_width = texture_array.shape[:2]
    x_tex = np.rint(hit_uv[:, 0] * (tex_width - 1)).astype(np.int32)
    y_tex = np.rint((1 - hit_uv[:, 1]) * (tex_height - 1)).astype(np.int32)
    x_tex = np.clip(x_tex, 0, tex_width - 1)
    y_tex = np.clip(y_tex, 0, tex_height - 1)
    hit_colors = texture_array[y_tex, x_tex]
    
    # Fill the sampled colors into the corresponding positions
    global_hit_indices = valid_indices[index_ray]
    patch_color = np.zeros((num_points, 3), dtype=np.uint8)
    patch_mask = np.zeros(num_points, dtype=np.uint8)
    patch_color[global_hit_indices] = hit_colors
    patch_mask[global_hit_indices] = 255
    
    # Generate and save the output image and mask
    patch_color_img = patch_color.reshape((grid_size, grid_size, 3))
    patch_mask_img = patch_mask.reshape((grid_size, grid_size))    
    patch_mask_img_inpainted = cv2.inpaint(patch_color_img, (255-patch_mask_img), inpaintRadius=1, flags=cv2.INPAINT_TELEA)
    
    if plot:
        plt.figure(figsize=(8, 4))
        plt.subplot(1, 3, 1)
        plt.imshow(patch_color_img)
        plt.title("Texture Patch")
        plt.axis('off')
        
        plt.subplot(1, 3, 2)
        plt.imshow(patch_mask_img, cmap='gray')
        plt.title("Mask")
        plt.axis('off')
        
        plt.subplot(1, 3, 3)
        plt.imshow(patch_mask_img_inpainted)
        plt.title("Inpainted")
        plt.axis('off')
        plt.show()
    
    return patch_color_img, patch_mask_img, patch_mask_img_inpainted

def extract_vgg_features(img_array, N=10, stats_type='sum'):
    """
    Extract the activation features of the first N layers of VGG16 and calculate the specified statistics.
    
    Args:
    - img_array (np.ndarray): HxWx3 uint8 image array
    - N (int): Number of layers to capture (default is 10)
    - stats_type (str): Type of statistics, options are 'sum', 'mean', 'var', 'l1', 'l2', default is 'sum'

    Returns:
    - activations_stats (dict): Statistics of each layer, the key is the layer name, and the value is the scalar value of the statistics.
    """
    assert isinstance(img_array, np.ndarray) and img_array.ndim == 3 and img_array.shape[2] == 3, \
        "The input image must be a HxWx3 uint8 numpy array."

    # Convert numpy array to PIL image
    img_pil = Image.fromarray(img_array.astype(np.uint8))

    # Image preprocessing
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

    # Preprocess and add batch dimension
    img_tensor = preprocess(img_pil).unsqueeze(0)  # Shape: [1, 3, 224, 224]

    # Load pre-trained VGG16 model
    model = models.vgg16(pretrained=True)
    model.eval()  # Switch to evaluation mode

    # Dictionary to store activation values
    activations = {}

    # Define a hook function to capture layer activation values
    def get_activation(name):
        def hook(model, input, output):
            activations[name] = output.detach()  # Do not backpropagate
        return hook

    # Register hooks to the first N layers
    for idx, layer in enumerate(model.features):
        if idx < N:
            layer.register_forward_hook(get_activation(f"layer_{idx}"))

    # Forward pass, automatically trigger hooks to capture activation values
    _ = model(img_tensor)

    # Calculate activation statistics
    activations_stats = {}
    for key, activation in activations.items():
        if stats_type == 'sum':
            stat_value = activation.sum().item()
        elif stats_type == 'mean':
            stat_value = activation.mean().item()
        elif stats_type == 'var':
            stat_value = activation.var().item()
        elif stats_type == 'l1':
            stat_value = activation.abs().sum().item()
        elif stats_type == 'l2':
            stat_value = torch.norm(activation, p=2).item()
        else:
            raise ValueError("Unsupported statistics type, please choose one of 'sum', 'mean', 'var', 'l1', 'l2'.")

        activations_stats[key] = stat_value

    return activations_stats

if __name__ == '__main__':

    ply_filename = "D:/Data/YCB_video/ycbv_models/models/obj_000001.ply"      # Your PLY file path
    texture_filename = "D:/Data/YCB_video/ycbv_models/models/obj_000001.png"  # Path to the texture image with the same name

    list_patch_color_img, list_patch_mask_img, list_patch_mask_img_inpainted = extract_texture_patch(ply_filename, texture_filename)
    
    

